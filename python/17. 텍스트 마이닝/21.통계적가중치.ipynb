{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "#긍정리뷰 100개 불러오기\n",
    "pos_review=(glob.glob(\"c:/data/imdb/train/pos/*.txt\"))[0:100]\n",
    "lines_pos=[]\n",
    "for i in pos_review:\n",
    "    try:\n",
    "        f = open(i, 'r')\n",
    "        temp = f.readlines()[0]\n",
    "        lines_pos.append(temp)\n",
    "        f.close()\n",
    "    except :\n",
    "        continue\n",
    "len(lines_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 3989)\n",
      "[[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.06538462 0.         ... 0.         0.         0.        ]\n",
      " [0.         0.23078109 0.         ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tokenizer = RegexpTokenizer('[\\w]+')\n",
    "stop_words = stopwords.words('english')\n",
    "#TF-IDF 가중치 할당\n",
    "vec = TfidfVectorizer(stop_words=stop_words)\n",
    "vector_lines_pos = vec.fit_transform(lines_pos)\n",
    "A=vector_lines_pos.toarray()\n",
    "print(A.shape)\n",
    "print(A)\n",
    "#x축 단어, y축 문서"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3989, 100)\n",
      "[[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.06538462 0.23078109]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "#현재 상태는 100개의 문서의 유사도\n",
    "#단어간의 유사도를 구하는 것이 목적이므로\n",
    "#단어-문서 행렬로 변경\n",
    "#x축 문서, y축 단어\n",
    "A=A.transpose()\n",
    "print(A.shape)\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t0.5\n",
      "  (1, 1)\t1.0\n",
      "  (2, 0)\t0.7\n",
      "  (2, 2)\t1.5\n",
      "[[0.5 0.  0. ]\n",
      " [0.  1.  0. ]\n",
      " [0.7 0.  1.5]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "#밀집행렬(dense array)\n",
    "a=np.array([[0.5,0,0],[0,1,0],[0.7,0,1.5]])\n",
    "#밀집행렬을 희소행렬(sparse array)로 변환\n",
    "#밀집행렬의 단점: 0이 많을 경우 메모리 낭비가 될 수 있음\n",
    "#희소행렬은 0이 아닌 값들의 위치와 값만 기록하여 메모리를 절약하는 방식\n",
    "b=sparse.csr_matrix(a)\n",
    "print(b)\n",
    "# (0,0) 0.5 => 인덱스 0,0에 값 0.5\n",
    "# (1,1) 1 => 인덱스 1,1에 값 1\n",
    "# (2,0) 0.7 => 인덱스 2,0에 값 0.7\n",
    "# (2,2) 1 => 인덱스 2,2에 값 1.5\n",
    "c=b.toarray() #희소행렬을 밀집행렬로 변환\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((1556, 108), 0.1025903571594213),\n",
       " ((1557, 108), 0.1397555705460018),\n",
       " ((1563, 108), 0.304841238021036),\n",
       " ((1574, 108), 0.304841238021036),\n",
       " ((1575, 108), 0.1373985754191761),\n",
       " ((1588, 108), 0.08716652865566843),\n",
       " ((1589, 108), 0.304841238021036),\n",
       " ((1598, 108), 0.12432181608698951),\n",
       " ((1606, 108), 0.1835286370686118),\n",
       " ((1607, 108), 0.304841238021036)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "A_sparse = sparse.csr_matrix(A)\n",
    "#코사인 유사도 계산\n",
    "similarities_sparse = cosine_similarity(A_sparse,dense_output=False)\n",
    "# todok() 행렬을 딕셔너리 형태로 변환\n",
    "list(similarities_sparse.todok().items())[35000:35010]\n",
    "#list(similarities_sparse.todok().items())[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'friday'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#단어 이름이 아닌 인덱스 형태로 출력됨\n",
    "# (1469,108), 0.37 1469 단어와 108 단어의 유사도는 37%\n",
    "vec.get_feature_names_out()[1469]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(2613, 3250)</td>\n",
       "      <td>0.499980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(3250, 2613)</td>\n",
       "      <td>0.499980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(2545, 1188)</td>\n",
       "      <td>0.499979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(1188, 2545)</td>\n",
       "      <td>0.499979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(1767, 2543)</td>\n",
       "      <td>0.499965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(2543, 1767)</td>\n",
       "      <td>0.499965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(1074, 250)</td>\n",
       "      <td>0.499935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(250, 1074)</td>\n",
       "      <td>0.499935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(2322, 525)</td>\n",
       "      <td>0.499934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(525, 2322)</td>\n",
       "      <td>0.499934</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          words    weight\n",
       "0  (2613, 3250)  0.499980\n",
       "1  (3250, 2613)  0.499980\n",
       "2  (2545, 1188)  0.499979\n",
       "3  (1188, 2545)  0.499979\n",
       "4  (1767, 2543)  0.499965\n",
       "5  (2543, 1767)  0.499965\n",
       "6   (1074, 250)  0.499935\n",
       "7   (250, 1074)  0.499935\n",
       "8   (2322, 525)  0.499934\n",
       "9   (525, 2322)  0.499934"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#위의 결과값을 데이터프레임으로 출력\n",
    "df=pd.DataFrame(list(similarities_sparse.todok().items()),columns=[\"words\",\"weight\"])\n",
    "df2=df.sort_values(by=['weight'],ascending=False)\n",
    "df2=df2.reset_index(drop=True)\n",
    "#단어 자신끼리의 짝은 1이 되므로 1보다 작은 항목들만 출력시킴\n",
    "df3=df2.loc[np.round(df2['weight']) < 1]\n",
    "df3=df3.reset_index(drop=True)\n",
    "df3.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "physically,society=>0.50\n",
      "society,physically=>0.50\n",
      "past,essence=>0.50\n",
      "essence,past=>0.50\n",
      "iceberg,passengers=>0.50\n",
      "passengers,iceberg=>0.50\n",
      "dying,art=>0.50\n",
      "art,dying=>0.50\n",
      "move,care=>0.50\n",
      "care,move=>0.50\n",
      "horner,long=>0.50\n",
      "long,horner=>0.50\n"
     ]
    }
   ],
   "source": [
    "for i,row in enumerate(df3.iterrows()):\n",
    "    a=vec.get_feature_names_out()[row[1][0][0]]\n",
    "    b=vec.get_feature_names_out()[row[1][0][1]]\n",
    "    print(f'{a},{b}=>{row[1][1]:.2f}')    \n",
    "    if i>10:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ff4f85d6e04298634172ac5d8264e7e9b556b95639fe52ebb9425c4d4cba0c9c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
